---
description: flows
globs: 
alwaysApply: false
---
# Disco-Musica: System Architecture and Flows

## Table of Contents
- [Overview](mdc:#overview)
  - [Core System Ctem-components)
  - [Design Philosophy](mdc:#design-philosophy)
- [System Architecture](mdc:#system-architecture)
  - [Core Layers](mdc:#[Data Fta-flowifified-Resouresouws)
 (#f - [P-02-p- [Au(#aud[Prow-antks](mdc:#flow-2-generate-new-tracks-following-song-harmony)
    - [Tune Vocals](mdc:#flow-3-tune-vocals)
  - [Model Training and Inference Flows](mdc:#model-training-and-inference-flows)
    - [Generation Model](mdc:#flow-4-generation-model-training-and-inference)
    - [Production Model](mdc:#flow-5-production-model-training-and-inference)
    - [Mastering Model](mdc:#flow-6-mastering-model-training-and-inference)
- [Logic Pro Integration](mdc:#logic-pro-integration)
  - [Plugin Architecture](mdc:#plugin-architecture)
  - [Communication Protocol](mdc:#communication-protocol)
  - [Plugin Components](mdc:#plugin-components)
  - [Integration with Flows](mdc:#integration-with-flows)
- [Universal Data Architecture](mdc:#universal-data-architecture)
  - [Multi-Model Data Store](mdc:#multi-model-data-store)
  - [Core Resource Types](mdc:#core-resource-types)
  - [Data Access Patterns](mdc:#data-access-patterns)
  - [Scalability Considerations](mdc:#scalability-considerations)
- [Training Data Requirements](mdc:#training-data-requirements)
- [Inference Methods](mdc:#inference-methods)
- [User Interface (UI) Guidelines](mdc:#user-interface-ui-guidelines)

## Overview

Disco-Musica employs a modular architecture with a hybrid computing approach, designed for maximum data efficiency, universal access, and scalability.

### Core System Components

- **Universal Data Layer**: Centralized data repository with standardized access protocols
- **Unified Resource Management**: System for tracking and accessing all project resources
- **Hybrid Compute Architecture**: 
  - Local inference for real-time interaction
  - Cloud-based training (Google Colab, AWS, Azure) for intensive computation
- **Extensible Model Framework**: Plug-and-play model integration system
- **Modular UI**: Adaptable interface components for different user workflows

### Design Philosophy

- **Data Efficiency**: All data generated during any process is preserved and made available to other modules
- **Universal Access**: Standardized API for all data and model access
- **Scalability**: Components designed to scale horizontally with increasing data and users
- **Modularity**: Self-contained components that can evolve independently
- **Interoperability**: Consistent interfaces between all system components

## System Architecture

The architecture follows a combination of layered and service-oriented patterns:

### Core Layers

1. **Data Foundation Layer**
   - Universal project repository
   - Standardized data formats and access patterns
   - Resource tracking and indexing
   - Versioning and history
   - Caching and performance optimization

2. **Service Layer**
   - **Generation Service**: Creates musical content from various inputs
   - **Model Service**: Manages model lifecycle and execution
   - **Output Service**: Handles result processing and presentation
   - **Resource Service**: Coordinates access to all system resources
   - **Integration Service**: Manages external tool connections (Logic Pro, etc.)

3. **Application Layer**
   - User interface components
   - Workflow orchestration
   - Session management
   - User preferences and customization

### Data Flow Patterns

Each operation follows a consistent pattern:

1. **Resource Acquisition**: Identify and access required inputs
2. **Processing Pipeline**: Apply transformations through configurable stages
3. **Result Persistence**: Store all outputs in the universal data layer
4. **Notification**: Alert dependent components of new or updated resources

This pattern ensures maximum data reuse, consistent access patterns, and clear dependencies between operations.

## Unified Process Flows

The following flow diagrams illustrate the key operational sequences in the Disco-Musica system. Each flow follows the universal resource pattern, where every operation:

1. Consumes resources from the universal data layer
2. Processes them according to a specific workflow
3. Produces new resources that are added back to the data layer
4. Makes these resources available to all other system components

### Resource Management Flows
graph TD
    A[Logic Project Input] --> B[Export WAV and MIDI to Root Folder];
    B --> C[Create New Project Directory];
    C --> D[Organize WAV and MIDI into Project Directory];
    D --> E{Choose Flow: Profile Update or Import};
    E -- Profile Update --> F1[Extract Project Data (from WAV/MIDI)];
    E -- Project Import --> G1[Create Project Entry in DB];
    G1 --> F1;
    F1 --> H1[Audio Data Processing];
    F1 --> I1[MIDI Data Processing];
    H1 --> J1[Chord/Harmonic Analysis];
    I1 --> J1;
    J1 --> K1[Genre/Emotion/Style Analysis];
    K1 --> L1[Store Analysis Results/Update Project Profile];
    L1 --> M1[End];
 Flow Descriptions:

Flow #0.1: Profile Updating

Purpose: Extract and analyze project data to maintain up-to-date project profiles.

Input: A folder in the root directory containing WAV and MIDI files.

Process:

Export WAV and MIDI: User exports all audio tracks as WAV files and MIDI data as MIDI files into a designated folder in the root directory.
Create New Project Directory: The application creates a new, empty directory to organize the project.
Organize Files: The application moves and organizes the exported WAV and MIDI files from the root directory folder into the newly created project directory, ensuring proper file naming and structure.
Data Extraction (from WAV/MIDI):
Extract project metadata (BPM, key, time signature - if possible from file naming or user input)
MIDI data (notes, velocity, timing) from MIDI files.
Audio data (waveforms, transients) from WAV files.
Feature Extraction and Preprocessing:
Audio Processing:
Extract acoustic features (MFCCs, spectral centroid, chroma features).
Remove silence and normalize volume.
Convert to frequency domain representations when needed.
MIDI Processing:
Quantize notes and normalize velocity.
Convert to piano-roll representation for analysis.
Extract chord progressions and harmonic structures.
Project Context:
Calculate average track volume levels.
Determine track density and arrangement complexity.
Musical Analysis:
Chord/Harmonic Analysis:
Use Librosa and Music21 for chord detection.
Extract harmonic progression patterns.
Identify key changes and modulations.
Genre/Style Classification:
Apply pre-trained models for genre detection.
Analyze emotional content based on musical features.
Identify stylistic elements and production techniques.
Database Update:
Store extracted data and analysis results.
Update project profile with new information.
Link related assets and generated resources.
Output: Updated project profile in the database.

Modified Flow #0.2: Project Importing

Purpose: Import a new project, extract its data, and create a structured project environment.

Input: A folder in the root directory containing WAV and MIDI files.

Process:

Export WAV and MIDI: User exports all audio tracks as WAV files and MIDI data as MIDI files into a designated folder in the root directory.
Create New Project Directory: The application creates a new, empty directory to organize the project.
Organize Files: The application moves and organizes the exported WAV and MIDI files from the root directory folder into the newly created project directory, ensuring proper file naming and structure.
Database Entry Creation:
Generate unique project identifier.
Create database entry with basic metadata (if available from file names, or user input).
Initialize project status and relationships.
Data Extraction (from WAV/MIDI):
Extract project metadata (BPM, key, time signature - if possible from file naming or user input)
MIDI data (notes, velocity, timing) from MIDI files.
Audio data (waveforms, transients) from WAV files.
Feature Extraction and Preprocessing:
Process audio, MIDI, and project context.
Storage and Organization:
Store original and processed files.
Create references between database and file system.
Generate metadata indexes for quick access.
Initial Analysis:
Perform chord/harmonic analysis.
Complete genre/emotion/style classification.
Store analysis results in database and project directory.
Output: Complete project profile in database and organized project directory.

### Audio Processing Flows

#### Flow #1: Project Macro Quantizing

**Purpose**: Ensure rhythmic consistency across the project by aligning audio and MIDI to a consistent grid

**Input**: Logic project file

**Process**:
1. **BPM Standardization**
   - Analyze tempo variations throughout the project
   - Establish a consistent BPM value
   - Apply tempo mapping if needed for tempo changes

2. **MIDI Quantization**
   - Apply intelligent quantization to MIDI tracks
   - Align notes to the nearest grid divisions
   - Preserve intentional timing variations when appropriate
   - Adjust note durations to match quantized start points

3. **Audio Transient Alignment**
   - Detect audio transients using onset detection algorithms
   - Use flex editing to align transients to the grid
   - Apply time stretching algorithms that preserve audio quality
   - Handle polyphonic audio with multi-band processing

4. **Project Timeline Adjustment**
   - Shift the entire project to start at the second time marker
   - Ensure all tracks are properly aligned to this reference point
   - Add pre-roll if necessary for processing headroom

5. **Project Saving**
   - Save the quantized project with appropriate metadata
   - Create backup of original project if requested
   - Generate log of applied quantization operations

**Output**: Quantized Logic project file

**Technical Implementation**:
- **Audio Analysis**: Librosa onset detection
- **Time Stretching**: Elastique or similar high-quality algorithms
- **MIDI Processing**: Mido, Music21

```mermaid
graph TD
    A[Logic Project Input] --> B[Ensure Consistent BPM];
    B --> C[Quantize MIDI Tracks];
    C --> D[Quantize Audio Transients];
    D --> E[Shift Project Start];
    E --> F[Save Quantized Project];
```

#### Flow #2: Generate New Tracks Following Song Harmony

**Purpose**: Create new musical elements that complement the existing harmony structure

**Input**: Logic project file, analyzed harmony

**Process**:
1. **Harmony Analysis**
   - Extract chord progressions and harmonic structures
   - Identify key centers and modulations
   - Determine harmonic rhythm (chord change rate)
   - Create harmonic context map for generative guidance

2. **Data Preprocessing**
   - Convert harmony data to model-compatible representations
   - Prepare conditioning parameters (genre, style, emotion)
   - Generate embedding vectors for harmonic context

3. **Generative Model Application**
   - **Vocal Melody Generation**
     - Generate 5 variations using transformer-based models
     - Apply constraints for singability and range
     - Ensure melodic coherence with harmony
   - **Lead Line Generation**
     - Generate 5 variations with instrumental focus
     - Optimize for instrument-specific playability
     - Ensure stylistic consistency
   - **Harmonizing Melody Generation**
     - Generate 5 variations with multi-voice harmony
     - Balance counterpoint and harmonic support
     - Implement voice-leading principles
   - **Drum Pattern Generation**
     - Generate 5 variations with appropriate groove
     - Align with project tempo and feel
     - Adapt to genre expectations

4. **MIDI Integration**
   - Convert generated ideas to MIDI format
   - Apply appropriate MIDI CC data for expression
   - Organize generated tracks within Logic project
   - Assign suitable virtual instruments

**Output**: Logic project with new MIDI tracks

**Technical Implementation**:
- **Harmony Analysis**: Music21, custom chord detection
- **Generative Models**: Transformer-based MIDI generation (MusicGen, MusicLM)
- **MIDI Integration**: Mido, custom MIDI utilities

```mermaid
graph TD
    A[Logic Project, Harmony Input] --> B[Analyze Harmony];
    B --> C1[Prepare Harmony Data for Models];
    C1 --> C[Generate Vocal MIDI 5 variations];
    C1 --> D[Generate Lead Line MIDI 5 variations];
    C1 --> E[Generate Harmonizing MIDI 5 variations];
    C1 --> F[Generate Drumming MIDI 5 variations];
    C --> G[Add MIDI to Project];
    D --> G;
    E --> G;
    F --> G;
```

#### Flow #3: Tune Vocals

**Purpose**: Apply intelligent pitch correction to vocal tracks that respects musical context

**Input**: Vocal audio track, project key, harmony

**Process**:
1. **Pitch Analysis**
   - Detect fundamental frequencies throughout vocal track
   - Identify note transitions and pitch drift
   - Map current pitch against target notes
   - Create pitch correction map

2. **Preprocessing**
   - Separate vocal from background elements if needed
   - Normalize volume for consistent processing
   - Apply pre-filtering for improved pitch detection
   - Segment vocal into phrases for targeted correction

3. **Auto-Tune Application**
   - Apply context-aware pitch correction using:
     - Project key information
     - Current chord progressions
     - Harmonic context at each point
   - Generate multiple tuned versions with varying parameters:
     - Correction strength (subtle to robotic)
     - Correction speed (natural to instant)
     - Formant preservation settings
     - Special effects (quantization artifacts, vibrato)

4. **Results Integration**
   - Add processed vocal tracks to the project
   - Organize variations with descriptive naming
   - Maintain processing metadata for later adjustments
   - Apply appropriate channel strip settings

**Output**: Logic project with tuned vocal tracks

**Technical Implementation**:
- **Pitch Detection**: CREPE or pYIN algorithms
- **Pitch Correction**: Custom implementation with PyTorch
- **Audio Processing**: Librosa, PyDub

```mermaid
graph TD
    A[Vocal Track, Key, Harmony Input] --> B[Analyze Vocal Pitch];
    B --> C1[Vocal Preprocessing];
    C1 --> C[Apply Auto-Tune Multiple Versions];
    C --> D[Add Tuned Vocals to Project];
```

### Model Training and Inference Flows

#### Flow #4: Generation Model Training and Inference

**Purpose**: Create and apply models that can generate musical content from various inputs

**Training Process**:
1. **Data Collection and Curation**
   - Collect diverse training data:
     - Logic project files
     - Audio files (multiple genres/styles)
     - MIDI sequences
     - Lyrics and text descriptions
   - Apply quality filtering and deduplication
   - Create training/validation/test splits

2. **Data Preprocessing and Feature Extraction**
   - **Audio Processing**
     - Convert to consistent format (44.1kHz, mono/stereo)
     - Extract spectral features (Mel spectrograms, MFCCs)
     - Apply normalization and standardization
     - Create augmented versions (pitch shift, time stretch)
   - **MIDI Processing**
     - Standardize format and structure
     - Create token sequences for model input
     - Apply data augmentation techniques
   - **Text Processing**
     - Clean and normalize text content
     - Apply tokenization for model input
     - Generate embeddings using pre-trained models

3. **Model Architecture Selection**
   - **For MIDI Generation**:
     - Transformer-based sequence models
     - Recurrent neural networks with attention
   - **For Audio Generation**:
     - Vector Quantized VAE (VQ-VAE)
     - WaveNet or similar autoregressive models
     - Diffusion models for high-quality synthesis
   - **For Project Structure**:
     - Graph Neural Networks for structure learning
     - Transformers for sequential relationships

4. **Training Configuration**
   - **Hyperparameters**:
     - Learning rates: Full training (5e-4 to 1e-3), Fine-tuning (1e-5 to 5e-5)
     - Batch sizes: 16-128 depending on model complexity
     - Optimization algorithm: AdamW with weight decay
   - **Training Context**:
     - Include musical parameters (key, tempo, chord structure)
     - Account for arrangement and instrumentation
     - Incorporate stylistic and genre information
   - **Compute Environment**:
     - Google Colab for accessible GPU/TPU access
     - Checkpoint saving and resumable training

5. **Training Execution and Monitoring**
   - Execute training loops with validation
   - Monitor key metrics (loss curves, audio quality)
   - Implement early stopping and learning rate scheduling
   - Save checkpoints and model versions

**Inference Process**:
1. **Input Processing**
   - Accept diverse input modalities:
     - Text descriptions
     - Audio examples (full or partial)
     - MIDI sequences
     - Images or video for creative inspiration
   - Convert to model-compatible representations
   - Apply conditioning parameters as needed

2. **Model Selection and Application**
   - Select appropriate pre-trained or fine-tuned model
   - Configure generation parameters:
     - Temperature and sampling strategy
     - Output length and structure
     - Stylistic controls
   - Run inference pipeline with optimized settings

3. **Output Generation and Post-processing**
   - Generate raw output from model
   - Convert to appropriate format (MIDI or audio)
   - Apply post-processing for quality improvement
   - Format for integration with Logic project

**Output**: Generated MIDI or audio files

**Technical Implementation**:
- **Training Framework**: PyTorch (primary), TensorFlow (supported)
- **Model Architectures**: Transformers, VAEs, Diffusion models
- **Optimization**: Model quantization, ONNX runtime
- **Inference**: Local CPU/GPU inference, API integration

```mermaid
graph TD
    subgraph "Training Flow"
    A1[Project Directory] --> B1[Import Logic/Audio/MIDI];
    B1 --> C1[Data Preprocessing];
    C1 --> D1[Feature Extraction];
    D1 --> E1[Tokenize Data];
    E1 --> F1[Train Generative Model];
    F1 --> G1[Save Trained Model];
    end
    
    subgraph "Inference Flow"
    A2[Input Text/Audio/MIDI/Image/Video] --> B2[Input Preprocessing];
    B2 --> C2[Feature Extraction];
    C2 --> D2[Generate Music];
    D2 --> E2[Post-processing];
    E2 --> F2[Convert to MIDI/Audio];
    end
```

#### Flow #5: Production Model Training and Inference

**Purpose**: Create and apply models that predict professional-quality effect chains for audio tracks

**Training Process**:
1. **Data Collection**
   - Gather diverse audio source material:
     - Raw audio recordings
     - Applied effect chains data
     - Mixing session parameters
     - Sound design examples
   - Organize by instrument type and genre
   - Create paired samples (before/after processing)

2. **Data Preprocessing**
   - **Audio Feature Extraction**
     - Extract spectral and temporal features
     - Generate instrument-specific features
     - Create context-aware features (track role)
   - **Effect Parameter Normalization**
     - Standardize effect parameters across plugins
     - Create normalized parameter spaces
     - Map continuous and discrete parameters
   - **Data Augmentation**
     - Apply controlled variations to source audio
     - Create training variations with different parameters
     - Ensure diverse representation of mixing scenarios

3. **Model Architecture Selection**
   - Regression models for parameter prediction
   - Neural networks for complex effect modeling
   - Sequence models for temporal processing effects

4. **Training Configuration**
   - Configure hyperparameters for various effect types
   - Implement loss functions for audio quality assessment
   - Establish validation methods for subjective quality

5. **Training Execution**
   - Train specialized models for different instrument types
   - Implement curriculum learning for complex effects
   - Save and version models for different use cases

**Inference Process**:
1. **Audio Analysis**
   - Analyze input audio track characteristics
   - Identify instrument type and playing style
   - Extract relevant features for effect prediction

2. **Effect Chain Generation**
   - Select appropriate pre-trained models
   - Generate effect chain parameters based on audio input
   - Consider track type and musical context
   - Create multiple parameter variations

3. **Effect Application**
   - Apply generated effect chain to audio track copy
   - Process through Logic Pro's built-in plugins
   - Create multiple versions with parameter variations
   - Enable parameter fine-tuning via UI

**Output**: Audio track with applied effect chain

**Technical Implementation**:
- **Audio Analysis**: Librosa, PyDub
- **Model Implementation**: PyTorch, SciKit-Learn
- **Plugin Integration**: Logic Pro API or scripting

```mermaid
graph TD
    subgraph "Training Flow"
    A1[Audio/Effect Data] --> B1[Audio Feature Extraction];
    B1 --> C1[Effect Parameter Normalization];
    C1 --> D1[Data Augmentation];
    D1 --> E1[Train Production Model];
    E1 --> F1[Save Trained Model];
    end
    
    subgraph "Inference Flow"
    A2[Audio Track, Track Type] --> B2[Analyze Audio];
    B2 --> C2[Feature Extraction];
    C2 --> D2[Generate Effect Chain];
    D2 --> E2[Apply Effect Chain to Copy];
    E2 --> F2[Enable Parameter Tuning];
    end
```

#### Flow #6: Mastering Model Training and Inference

**Purpose**: Create and apply models that automate professional-quality mastering for audio projects

**Training Process**:
1. **Data Collection**
   - Gather paired audio examples:
     - Pre-mastered mixes
     - Professionally mastered versions
     - Mastering settings and parameters
   - Organize by genre, style, and loudness target
   - Document mastering chain configurations

2. **Data Preprocessing**
   - **Audio Feature Extraction**
     - Extract mastering-relevant features (dynamic range, spectral balance)
     - Calculate loudness metrics (LUFS, true peak)
     - Generate genre-specific reference features
   - **Mastering Parameter Normalization**
     - Standardize parameters across different tools
     - Create normalized parameter spaces
     - Map continuous and discrete parameters
   - **Data Augmentation**
     - Create variations with different input levels
     - Apply controlled distortion to test robustness
     - Generate challenging examples (problematic mixes)

3. **Model Architecture and Training**
   - Develop regression models for parameter prediction
   - Implement specialized models for different genres
   - Train with perceptual audio quality metrics
   - Validate against professional reference masters

**Inference Process**:
1. **Mix Analysis**
   - Analyze the final mix characteristics
   - Measure loudness, dynamic range, spectral balance
   - Identify potential issues (excess bass, harshness)
   - Compare to reference tracks if available

2. **Mastering Parameter Generation**
   - Select appropriate pre-trained model
   - Generate mastering chain parameters
   - Create multiple parameter sets for different targets
   - Optimize for platform-specific delivery (streaming, CD)

3. **Mastering Application**
   - Apply generated mastering chain using Logic Pro plugins
   - Process the mix through the complete chain
   - Provide interactive controls for parameter adjustment
   - Generate multiple masters for different platforms

**Output**: Mastered Logic project

**Technical Implementation**:
- **Audio Analysis**: Loudness analysis (LUFS measurement)
- **Parameter Prediction**: Regression models, neural networks
- **Plugin Integration**: Logic Pro API or scripting

```mermaid
graph TD
    subgraph "Training Flow"
    A1[Mastered Audio/Settings] --> B1[Extract Audio Features];
    B1 --> C1[Normalize Mastering Parameters];
    C1 --> D1[Apply Data Augmentation];
    D1 --> E1[Train Mastering Model];
    E1 --> F1[Save Trained Model];
    end
    
    subgraph "Inference Flow"
    A2[Logic Project Mix] --> B2[Analyze Mix];
    B2 --> C2[Extract Audio Features];
    C2 --> D2[Generate Mastering Parameters];
    D2 --> E2[Apply Mastering Chain Logic];
    E2 --> F2[Parameter Fine Tuning];
    end
```

## Universal Data Architecture

The Disco-Musica system implements a flexible, scalable data architecture designed for maximum interoperability, performance, and future extensibility.

### Multi-Model Data Store

Rather than relying on a single database technology, the system employs a multi-model approach:

1. **Document Store**
   - Technology: MongoDB
   - Purpose: Flexible schema for project metadata, analysis results, and parameters
   - Advantages: Schema flexibility, query performance for complex nested data

2. **Relational Database**
   - Technology: PostgreSQL
   - Purpose: Structured relationships, transactional integrity, complex queries
   - Advantages: ACID compliance, robust indexing, complex joins

3. **Vector Database**
   - Technology: Pinecone or Milvus
   - Purpose: Semantic search and similarity matching for musical elements
   - Advantages: High-dimensional vector operations, efficient similarity search

4. **Time-Series Database**
   - Technology: InfluxDB or TimescaleDB
   - Purpose: Performance metrics, model training history, usage analytics
   - Advantages: Optimized for time-based data, efficient aggregation

5. **File System Integration**
   - Technology: Object storage with metadata indexing
   - Purpose: Large binary assets (audio files, model weights)
   - Advantages: Cost-effective storage, content-addressed retrieval

### Core Resource Types

All resources in the system follow a consistent metadata pattern:

**Base Resource Properties**
- `resource_id`: UUID uniquely identifying any resource
- `resource_type`: Type identifier (project, track, model, etc.)
- `creation_timestamp`: When the resource was created
- `modification_timestamp`: When the resource was last modified 
- `version`: Incremental version number
- `parent_resources`: List of resources that were inputs to this resource
- `tags`: Flexible key-value pairs for categorization
- `access_control`: Permissions and sharing information

**Resource-Specific Data Models**

#### Project Resources
```json
{
  "basic_info": {
    "name": "Project Name",
    "description": "Project description",
    "daw_type": "logic_pro",
    "original_file_path": "/path/to/logic/file.logicx"
  },
  "musical_properties": {
    "bpm": 120.5,
    "key": "C major",
    "time_signature": "4/4",
    "genre": ["pop", "electronic"],
    "emotion": ["energetic", "uplifting"],
    "style_tags": ["modern", "synthesizer-heavy"]
  },
  "tracks": ["track_resource_id_1", "track_resource_id_2", "..."],
  "analysis_results": ["analysis_resource_id_1", "analysis_resource_id_2", "..."],
  "generation_results": ["generation_resource_id_1", "generation_resource_id_2", "..."],
  "modification_history": [
    {
      "timestamp": "2023-01-01T12:00:00Z",
      "operation": "quantize",
      "parameters": {"strength": 0.8, "note_value": "1/16"},
      "resulting_resources": ["resource_id_1", "resource_id_2"]
    }
  ]
}
```

#### Track Resources
```json
{
  "basic_info": {
    "name": "Track Name",
    "type": "audio | midi | instrument",
    "role": "vocal | lead | bass | drums | ...",
    "original_file_path": "/path/to/extracted/file.wav"
  },
  "content_reference": {
    "storage_type": "object_store | file_system | embedded",
    "location": "track_data/uuid/content.wav",
    "format": "wav | mp3 | midi | ...",
    "duration_seconds": 180.5,
    "checksum": "sha256:..."
  },
  "audio_properties": {
    "sample_rate": 44100,
    "bit_depth": 24,
    "channels": 2,
    "peak_amplitude": -3.2,
    "integrated_loudness": -14.2
  },
  "midi_properties": {
    "note_count": 432,
    "velocity_range": [10, 127],
    "pitch_range": [36, 84],
    "polyphony": true
  },
  "derived_features": {
    "feature_vectors": ["feature_resource_id_1", "feature_resource_id_2"],
    "extracted_audio_features": {
      "mfcc": "feature_resource_id_3",
      "chroma": "feature_resource_id_4"
    },
    "harmonic_analysis": "analysis_resource_id_1"
  },
  "processing_chain": [
    {
      "processor_type": "eq",
      "parameters": {"low_cut": 80, "high_shelf": {"freq": 10000, "gain": 2.0}},
      "position": 0
    },
    {
      "processor_type": "compressor",
      "parameters": {"threshold": -18.0, "ratio": 4.0, "attack": 5.0, "release": 50.0},
      "position": 1
    }
  ]
}
```

#### Analysis Result Resources
```json
{
  "analysis_type": "chord_detection | genre_classification | key_detection | ...",
  "source_resources": ["resource_id_1", "resource_id_2"],
  "model_used": "model_resource_id",
  "confidence_score": 0.95,
  "execution_time_ms": 234,
  "result_summary": {
    "detected_key": "C major",
    "confidence": 0.95
  },
  "detailed_results": {
    "storage_type": "embedded | reference",
    "data": { /* Analysis-specific JSON structure */ }
  },
  "visualization_resources": ["visualization_resource_id_1"]
}
```

#### Model Resources
```json
{
  "model_info": {
    "name": "Model Name",
    "type": "generation | classification | regression | ...",
    "task": "chord_detection | genre_classification | ...",
    "version": "1.0.0",
    "architecture": "transformer | cnn | ...",
    "framework": "pytorch | tensorflow | onnx"
  },
  "weights_reference": {
    "storage_type": "object_store | file_system",
    "location": "models/uuid/weights.pt",
    "size_bytes": 500000000,
    "checksum": "sha256:..."
  },
  "training_info": {
    "training_run": "training_run_resource_id",
    "dataset": "dataset_resource_id",
    "performance_metrics": {
      "accuracy": 0.92,
      "loss": 0.08
    }
  },
  "usage_requirements": {
    "minimum_compute": {
      "ram_gb": 8,
      "gpu_memory_gb": 4
    },
    "quantized_versions": ["model_resource_id_quantized"]
  },
  "inference_parameters": {
    "default_settings": {
      "temperature": 0.8,
      "top_k": 50,
      "top_p": 0.95
    },
    "parameter_ranges": {
      "temperature": {"min": 0.1, "max": 1.0, "step": 0.1}
    }
  }
}
```

#### Generated Content Resources
```json
{
  "generation_type": "midi_sequence | audio_track | effect_chain | ...",
  "source_resources": ["resource_id_1", "resource_id_2"],
  "model_used": "model_resource_id",
  "generation_parameters": {
    "temperature": 0.8,
    "conditioning": {
      "key": "C major",
      "chord_progression": ["C", "Am", "F", "G"]
    }
  },
  "content_reference": {
    "storage_type": "object_store | file_system | embedded",
    "location": "generations/uuid/content.mid",
    "format": "mid | wav | mp3 | ...",
    "checksum": "sha256:..."
  },
  "metadata": {
    "generation_time_ms": 2345,
    "variation_number": 3,
    "user_rating": 4.5
  }
}
```

### Data Access Patterns

The system implements several access patterns to optimize for different use cases:

1. **Direct Resource Access**
   - Resource ID-based lookup for known resources
   - Consistent across all resource types
   - High-performance caching

2. **Graph-based Navigation**
   - Traverse resource relationships (parents, children, derived)
   - Discover resource lineage and dependencies
   - Visualize resource creation workflows

3. **Search-based Discovery**
   - Full-text search across resource metadata
   - Semantic similarity search for musical elements
   - Filtered queries with complex conditions

4. **Batch Processing Access**
   - Efficient access patterns for large-scale operations
   - Streaming interfaces for processing without full loading
   - Parallel access optimizations

### Scalability Considerations

The data architecture includes several features to ensure scalability:

1. **Horizontal Scaling**
   - Sharding strategies for document and relational data
   - Distribution of vector operations across nodes
   - Partitioning for time-series data

2. **Caching Hierarchy**
   - In-memory cache for frequently accessed resources
   - Distributed cache for shared access
   - Content-based caching for immutable resources

3. **Asynchronous Processing**
   - Event-driven updates to derived data
   - Background indexing of new content
   - Lazy computation of expensive features

4. **Data Lifecycle Management**
   - Automatic archiving of infrequently used resources
   - Tiered storage for cost optimization
   - Version pruning policies

## Training Data Requirements

### Generation Model
- **Large MIDI Datasets**
  - Lakh MIDI Dataset
  - Custom curated MIDI collections by genre
- **Audio Datasets**
  - Million Song Dataset
  - FreeSound database
  - Custom recorded audio libraries
- **Logic Project Files**
  - User-contributed projects
  - Public domain examples
- **Lyrics Datasets**
  - Song lyrics collections
  - Text corpora for style learning

### Production Model
- **Audio Recordings**
  - Raw recordings with corresponding effect chains
  - Before and after processing examples
- **Sound Design Libraries**
  - Effect presets and parameter mappings
  - Instrument-specific processing templates
- **Mixing Sessions**
  - Complete mixing session data
  - Channel strip configurations

### Mastering Model
- **Before-and-After Mastering Pairs**
  - Pre-master mixes
  - Professional mastering examples
- **Mastering Settings**
  - Plugin parameter data
  - Chain configurations
  - Target loudness specifications

## Inference Methods

### Generation Methods
- **Sampling Strategies**
  - Temperature-controlled sampling
  - Top-k and nucleus (top-p) sampling
  - Beam search for structured output
- **Conditional Generation**
  - Harmony-constrained generation
  - Style-guided synthesis
  - Rhythm and tempo conditioning
- **Local Inference**
  - Disco-diffusion style inference
  - Optimized for real-time interaction
  - Progressive generation with feedback

### Production and Mastering Methods
- **Parameter Prediction**
  - Effect parameter regression
  - Chain configuration optimization
  - Target-driven parameter adjustment
- **Plugin Integration**
  - Logic Pro API/scripting for parameter control
  - Non-destructive effect application
  - Parameter visualization and adjustment
  - A/B comparison capabilities

## User Interface (UI) Guidelines

### Core UI Requirements
- **Project Management**
  - Intuitive project import and organization
  - Clear project status visualization
  - Easy access to project versions and history
- **Analysis Visualization**
  - Interactive chord and harmony charts
  - Spectrograms and waveform displays
  - Genre and style classification results
- **Editor Integration**
  - MIDI editing with generative suggestions
  - Audio editing with effect previews
  - Seamless Logic Pro integration

### Specialized UI Components
- **Generation Control**
  - Input type selection (text, audio, MIDI, image)
  - Parameter controls for generation models
  - Real-time feedback and iterative refinement
- **Parameter Control**
  - Intuitive sliders for effect parameters
  - Preset management for common configurations
  - A/B testing interface for comparison
- **Process Monitoring**
  - Clear progress indicators for long-running tasks
  - Process cancellation and pause controls
  - Detailed error messages and troubleshooting

### User Experience Principles
- Save and load user presets for models and effect chains
- Provide visual feedback for each processing step
- Enable parameter fine-tuning after automated generation
- Support for keyboard shortcuts and workflow optimization
- Clear documentation and contextual help

## Logic Pro Integration

The Disco-Musica system integrates directly with Logic Pro through a custom plugin that serves as a bridge between the AI-powered backend and Logic Pro projects. This integration enables the application of intelligent modifications to projects using only stock Logic Pro features.

### Plugin Architecture

#### Key Design Principles

1. **Stock Features Only**
   - Uses exclusively stock Logic Pro plugins and features
   - Ensures maximum compatibility and stability across different Logic Pro versions
   - Eliminates the need for users to purchase or install additional plugins
   - Provides consistent behavior across different user setups

2. **External AI Processing**
   - All complex AI computations occur in the external Disco-Musica application:
     - Project analysis (BPM, key, harmony detection)
     - MIDI generation and manipulation
     - Audio processing algorithms and feature extraction
     - Mixing and mastering parameter generation
   - The plugin receives processed instructions via a structured JSON protocol

3. **Separation of Concerns**
   - Plugin handles only Logic Pro integration and modification application
   - External application handles all ML/AI inference and complex processing
   - Clear boundaries between components for maintainability and performance

#### Hardware Acceleration

- Metal framework integration for Apple Silicon optimization
- GPU-accelerated processing where applicable for UI and data handling
- Optimized CPU usage for real-time performance

### Communication Protocol

#### Data Exchange Format

- **Standardized JSON Structures**
  - Project metadata (BPM, key, time signature, tracks)
  - MIDI modification instructions (note addition/deletion, velocity changes)
  - Audio effect parameter configurations
  - Automation data specifications
  - Track creation and modification instructions

- **Versioned Protocol**
  - Backward compatibility support
  - Clear schema definitions
  - Extensibility for future features

#### Communication Channels

- **Socket-based Communication**
  - Local TCP/IP socket for communication with the external application
  - Binary message framing for efficient data transfer
  - Heartbeat mechanism for connection monitoring

- **File-based Exchange**
  - JSON configuration files for persistent settings
  - Temporary file exchange for large data transfers
  - Structured directory for asset management

- **Error Handling**
  - Comprehensive error codes and descriptions
  - Graceful fallback mechanisms
  - Detailed logging for troubleshooting

### Plugin Components

#### MIDI Manipulation Module

- **Scripter-based Implementation**
  - Custom JavaScript routines for MIDI data manipulation
  - Real-time MIDI processing and transformation
  - Advanced MIDI effects (arpeggiators, chord generators)

- **MIDI Generation Capabilities**
  - Apply AI-generated MIDI patterns to new or existing tracks
  - Intelligent quantization based on musical context
  - Voice-leading and harmony-aware note modification

- **Technical Implementation**
  - Built on Logic Pro's Scripter MIDI FX framework
  - Optimized JavaScript execution
  - Efficient MIDI data structure handling

#### Audio Effect Control Module

- **Stock Plugin Manipulation**
  - Apply AI-generated parameters to stock Logic plugins
  - Create and manage complex effect chains
  - Implement intelligent preset selection and blending

- **Parameter Automation**
  - Generate and apply automation data for dynamic control
  - Time-aligned automation synced with musical events
  - Multi-parameter coordinated automation for complex effects

- **Technical Implementation**
  - AU Plugin architecture for deep Logic Pro integration
  - Parameter mapping system for abstract control
  - Template-based effect chain application

#### Project Modification Module

- **Track Management**
  - Create, delete, and organize tracks based on AI recommendations
  - Apply appropriate instrument and effect settings
  - Configure routing and send/return relationships

- **Global Project Parameters**
  - Modify BPM, key, and time signature
  - Apply project-wide settings
  - Tag and organize project for database integration

- **Technical Implementation**
  - Logic Pro API integration for structural modifications
  - Transaction-based changes for error resilience
  - Undo support for all modifications

#### User Interface Component

- **Control Panel**
  - Minimal, non-intrusive interface within Logic Pro
  - Status indicators for backend connection
  - Process control (start/stop/pause)
  - Quick access to common functions

- **Parameter Adjustment**
  - Fine-tuning controls for AI-generated parameters
  - A/B comparison for different versions
  - Customization options for user preferences

- **Technical Implementation**
  - Native macOS interface components
  - Responsive design for different window sizes
  - Accessibility compliance
  - Keyboard shortcut integration

### Integration with Flows

The Logic Pro plugin integrates with each of the core flows, serving as the execution layer for applying AI-generated changes to the Logic Pro project.

#### Flow #0.1-0.2: Project Management Integration

- **Data Extraction**
  - Plugin extracts project metadata, MIDI, and audio data
  - Sends structured data to the backend for analysis
  - Maintains synchronization between Logic project and database

- **Implementation Details**
  - Periodic project scanning for changes
  - Delta-based updates to minimize data transfer
  - Background processing to avoid UI interruptions

```mermaid
graph TD
    A[Logic Pro Project] --> B[Plugin Extraction Module];
    B --> C[JSON Project Data];
    C --> D[Backend Analysis];
    D --> E[Database Update];
    D --> F[Plugin Receives Analysis Results];
    F --> G[Display in Logic Pro UI];
```

#### Flow #1: Project Macro Quantizing Integration

- **Quantization Application**
  - Plugin receives quantization parameters from backend
  - Applies intelligent quantization to MIDI using Scripter
  - Uses Logic Pro's Flex Time for audio quantization
  - Maintains musical integrity with context-aware adjustments

- **Implementation Details**
  - Track-by-track processing for granular control
  - Preview functionality before committing changes
  - Multiple quantization templates based on musical style

```mermaid
graph TD
    A[Backend Quantization Analysis] --> B[Generate Quantization Parameters];
    B --> C[Send to Logic Plugin];
    C --> D[Apply MIDI Quantization via Scripter];
    C --> E[Apply Audio Quantization via Flex Time];
    D --> F[Update Logic Project];
    E --> F;
```

#### Flow #2: Generate New Tracks Integration

- **MIDI Track Creation**
  - Plugin receives generated MIDI data from backend
  - Creates new tracks with appropriate instruments
  - Applies necessary MIDI effects and parameters
  - Organizes tracks in logical arrangement

- **Implementation Details**
  - Template-based track creation for consistency
  - Intelligent naming and coloring for organizational clarity
  - Background loading of software instruments to minimize disruption

```mermaid
graph TD
    A[Backend Harmony Analysis] --> B[Generate MIDI Content];
    B --> C[Send to Logic Plugin];
    C --> D[Create New Tracks];
    D --> E[Import MIDI Data];
    E --> F[Apply Instrument Settings];
    F --> G[Organize Track Hierarchy];
```

#### Flow #3: Tune Vocals Integration

- **Pitch Correction Application**
  - Plugin applies pitch correction using Logic Pro's Flex Pitch
  - Receives note-by-note correction parameters from backend
  - Creates multiple versions with different correction intensities
  - Preserves natural vocal characteristics while correcting pitch

- **Implementation Details**
  - Note-level parameter control for precise correction
  - Formant preservation settings based on voice characteristics
  - Layer-based approach for non-destructive editing

```mermaid
graph TD
    A[Backend Vocal Analysis] --> B[Generate Pitch Correction Map];
    B --> C[Send to Logic Plugin];
    C --> D[Apply Flex Pitch Correction];
    D --> E[Create Multiple Versions];
    E --> F[Enable Version Comparison];
```

#### Flow #4-6: Model Inference Integration

- **Effect Chain Application**
  - Plugin receives parameter settings for stock Logic plugins
  - Applies complex effect chains to tracks
  - Configures send/return routing for spatial effects
  - Creates automation data for dynamic parameter control

- **Mastering Integration**
  - Applies mastering chain to output bus
  - Configures multi-band compression, limiting, and EQ
  - Creates platform-specific masters with appropriate settings

- **Implementation Details**
  - Template-based effect chains for efficiency
  - Parameter mapping to accommodate different plugin versions
  - Intelligent preset blending for unique sounds

```mermaid
graph TD
    A[Backend AI Processing] --> B[Generate Effect Parameters];
    B --> C[Send to Logic Plugin];
    C --> D[Configure Plugin Chain];
    D --> E[Apply Parameters to Plugins];
    E --> F[Generate Automation Data];
    F --> G[Apply to Logic Project];
```

#### Performance Considerations

- **Latency Management**
  - Asynchronous communication to prevent UI freezing
  - Prioritized processing queue for real-time operations
  - Background processing for non-time-critical tasks

- **Resource Optimization**
  - Intelligent memory management
  - Batch processing for multiple changes
  - CPU usage throttling during playback

- **Error Resilience**
  - Transaction-based modifications
  - Automatic recovery from connection failures
  - Comprehensive error logging and reporting