---
description: training guide
globs: 
alwaysApply: false
---
. Project Goals and Philosophy

Goal: Develop an open-source, multimodal AI application ("Disco Musica") for generating custom music based on trained artist style.

Design Philosophy:
Data Efficiency: Preserve and reuse all generated data.
Universal Access: Standardized API for data and model access.
Scalability: Design components for horizontal scaling.
Modularity: Create self-contained, independently evolving components.
Interoperability: Ensure consistent interfaces between components.
II. System Architecture

Hybrid Compute Architecture:
Local inference for real-time interaction.
Cloud-based training (Google Colab, AWS, Azure) for intensive computation.
Core Layers:
Data Foundation Layer:
Universal project repository.
Standardized data formats and access patterns.
Resource tracking, indexing, versioning, and caching.
Service Layer:
Generation Service: Creates musical content.
Model Service: Manages model lifecycle and execution.
Output Service: Handles result processing and presentation.
Resource Service: Coordinates resource access.
Integration Service: Manages external tool connections (e.g., Logic Pro).
Application Layer:
User interface components.
Workflow orchestration and session management.
User preferences and customization.
Data Flow Pattern:
Resource Acquisition: Access required inputs from the data layer.
Processing Pipeline: Apply transformations through configurable stages.
Result Persistence: Store all outputs in the universal data layer.
Notification: Alert dependent components of new/updated resources.
III. Unified Process Flows (Detailed)

Each flow consumes resources, processes them, produces new resources, and makes them available.

A. Resource Management Flows

Flow #0.1: Profile Updating
Purpose: Extract and analyze project data to maintain up-to-date project profiles.
Input: Logic project file (.logicx).
Process:
Data Extraction: Parse Logic project file (metadata, MIDI, audio, lyrics, channel info).
Feature Extraction and Preprocessing:
Audio: Acoustic features (MFCCs, spectral centroid, chroma), silence removal, normalization.
MIDI: Quantization, piano-roll conversion, chord/harmonic extraction.
Lyrics: Cleaning, tokenization, sentiment analysis.
Project Context: Track volume levels, density, complexity.
Musical Analysis: Chord/harmonic analysis (Librosa, Music21), genre/style classification (pre-trained models).
Database Update: Store extracted data and analysis results; update project profile.
Output: Updated project profile in the database.
Technical Implementation:
Core Libraries: Librosa, Music21, PyTorch.
ML Models: Genre classification, emotion detection, style analysis.
Database Integration: PostgreSQL or MongoDB.
Flow #0.2: Project Importing
Purpose: Import a new project, extract its data, and create a structured project environment.
Input: Logic project file (.logicx).
Process:
Database Entry Creation: Generate unique project ID; create database entry.
Data Extraction: (Same as Flow #0.1).
Feature Extraction and Preprocessing: (Same as Flow #0.1).
Project Directory Structure: Create standardized project directory; organize extracted files.
Storage and Organization: Store files; create references between database and file system; generate metadata indexes.
Initial Analysis: Chord/harmonic analysis, genre/emotion/style classification.
Output: Complete project profile in database and organized project directory.
Technical Implementation:
File Operations: Python's os and pathlib.
Data Processing: NumPy, Pandas, Librosa.
Database Operations: SQLAlchemy or PyMongo.
B. Audio Processing Flows

Flow #1: Project Macro Quantizing
Purpose: Ensure rhythmic consistency by aligning audio and MIDI to a consistent grid.
Input: Logic project file.
Process:
BPM Standardization: Analyze tempo variations; establish consistent BPM.
MIDI Quantization: Apply intelligent quantization to MIDI tracks.
Audio Transient Alignment: Detect audio transients; align transients to the grid.
Project Timeline Adjustment: Shift project start; ensure track alignment.
Project Saving: Save quantized project; create backup of original project.
Output: Quantized Logic project file.
Technical Implementation:
Audio Analysis: Librosa onset detection.
Time Stretching: Elastique or similar high-quality algorithms.
MIDI Processing: Mido, Music21.
Flow #2: Generate New Tracks Following Song Harmony
Purpose: Create new musical elements that complement the existing harmony structure.
Input: Logic project file, analyzed harmony.
Process:
Harmony Analysis: Extract chord progressions, harmonic structures; create harmonic context map.
Data Preprocessing: Convert harmony data; prepare conditioning parameters; generate embedding vectors.
Generative Model Application:
Vocal Melody Generation: Generate 5 variations (transformer-based models).
Lead Line Generation: Generate 5 variations.
Harmonizing Melody Generation: Generate 5 variations.
Drum Pattern Generation: Generate 5 variations.
MIDI Integration: Convert generated ideas to MIDI; apply MIDI CC data; organize tracks; assign virtual instruments.
Output: Logic project with new MIDI tracks.
Technical Implementation:
Harmony Analysis: Music21, custom chord detection.
Generative Models: Transformer-based MIDI generation (MusicGen, MusicLM).
MIDI Integration: Mido, custom MIDI utilities.
Flow #3: Tune Vocals
Purpose: Apply intelligent pitch correction to vocal tracks that respects musical context.
Input: Vocal audio track, project key, harmony.
Process:
Pitch Analysis: Detect fundamental frequencies; identify note transitions and pitch drift; create pitch correction map.
Preprocessing: Separate vocal from background; normalize volume; apply pre-filtering; segment vocal into phrases.
Auto-Tune Application: Apply context-aware pitch correction; generate multiple tuned versions with varying parameters.
Results Integration: Add processed vocal tracks; organize variations; maintain processing metadata.
Output: Logic project with tuned vocal tracks.
Technical Implementation:
Pitch Detection: CREPE or pYIN algorithms.
Pitch Correction: Custom implementation with PyTorch.
Audio Processing: Librosa, PyDub.
C. Model Training and Inference Flows

Flow #4: Generation Model Training and Inference
Purpose: Create and apply models that can generate musical content from various inputs.
Training Process:
Data Collection and Curation: Collect diverse training data (Logic project files, audio, MIDI, lyrics); apply quality filtering; create training/validation/test splits.
Data Preprocessing and Feature Extraction:
Audio: Format conversion, spectral feature extraction (Mel spectrograms, MFCCs), normalization, augmentation.
MIDI: Standardization, token sequences, augmentation.
Text: Cleaning, normalization, tokenization, embeddings.
Model Architecture Selection:
MIDI Generation: Transformer-based sequence models, RNNs with attention.
Audio Generation: VQ-VAE, WaveNet, diffusion models.
Project Structure: Graph Neural Networks, Transformers.
Training Configuration:
Hyperparameters: Learning rates (Full training: 5e-4 to 1e-3, Fine-tuning: 1e-5 to 5e-5), batch sizes (16-128), optimization algorithm (AdamW with weight decay).
Training Context: Include musical parameters (key, tempo, chord structure), arrangement and instrumentation, stylistic and genre information.
Compute Environment: Google Colab, checkpoint saving, resumable training.
Training Execution and Monitoring: Execute training loops; monitor metrics; implement early stopping and learning rate scheduling; save checkpoints.
Inference Process:
Input Processing: Accept diverse input modalities (text, audio, MIDI, images, video); convert to model-compatible representations.
Model Selection and Application: Select pre-trained or fine-tuned model; configure generation parameters (temperature, sampling strategy, output length, stylistic controls).
Output Generation and Post-processing: Generate raw output; convert to appropriate format (MIDI or audio); apply post-processing.
Output: Generated MIDI or audio files.
Technical Implementation:
Training Framework: PyTorch (primary), TensorFlow (supported).
Model Architectures: Transformers, VAEs, Diffusion models.
Optimization: Model quantization, ONNX runtime.
Inference: Local CPU/GPU inference, API integration.
Flow #5: Production Model Training and Inference
Purpose: Create and apply models that predict professional-quality effect chains for audio tracks.
Training Process:
Data Collection: Gather diverse audio source material (raw recordings, effect chains data, mixing session parameters, sound design examples); organize by instrument type and genre.
Data Preprocessing:
Audio Feature Extraction: Spectral and temporal features, instrument-specific features, context-aware features.
Effect Parameter Normalization: Standardize effect parameters; create normalized parameter spaces.
Data Augmentation: Apply controlled variations to source audio.
Model Architecture Selection: Regression models, neural networks, sequence models.
Training Configuration: Configure hyperparameters; implement loss functions for audio quality assessment.
Training Execution: Train specialized models; implement curriculum learning.
Inference Process:
Audio Analysis: Analyze input audio track characteristics; extract relevant features.
Effect Chain Generation: Select appropriate pre-trained models; generate effect chain parameters; create multiple parameter variations.
Effect Application: Apply generated effect chain to audio track copy; process through Logic Pro's built-in plugins; enable parameter fine-tuning.
Output: Audio track with applied effect chain.
Technical Implementation:
Audio Analysis: Librosa, PyDub.
Model Implementation: PyTorch, SciKit-Learn.
Plugin Integration: Logic Pro API or scripting.
Flow #6: Mastering Model Training and Inference
Purpose: Create and apply models that automate professional-quality mastering for audio projects.
Training Process:
Data Collection: Gather paired audio examples (pre-mastered mixes, professionally mastered versions); organize by genre, style, and loudness target.
Data Preprocessing:
Audio Feature Extraction: Mastering-relevant features (dynamic range, spectral balance), loudness metrics.
Mastering Parameter Normalization: Standardize parameters; create normalized parameter spaces.
Data Augmentation: Create variations with different input levels.
Model Architecture and Training: Develop regression models; implement specialized models; train with perceptual audio quality metrics.
Inference Process:
Mix Analysis: Analyze the final mix characteristics; measure loudness, dynamic range, spectral balance.
Mastering Parameter Generation: Select appropriate pre-trained model; generate mastering chain parameters; optimize for platform-specific delivery.
Mastering Application: Apply generated mastering chain using Logic Pro plugins; provide interactive controls for parameter adjustment.
Output: Mastered Logic project.
Technical Implementation:
Audio Analysis: Loudness analysis (LUFS measurement).
Parameter Prediction: Regression models, neural networks.
Plugin Integration: Logic Pro API or scripting.
IV. Universal Data Architecture

Multi-Model Data Store:
Document Store: MongoDB (flexible schema for project metadata, analysis results, and parameters).
Relational Database: PostgreSQL (structured relationships, transactional integrity).
Vector Database: Pinecone or Milvus (semantic search and similarity matching).
Time-Series Database: InfluxDB or TimescaleDB (performance metrics, model training history, usage analytics).
File System Integration: Object storage with metadata indexing (large binary assets).
Core Resource Types:
Base Resource Properties: resource_id, resource_type, creation_timestamp, modification_timestamp, version, parent_resources, tags, access_control.
Resource-Specific Data Models: Detailed JSON schemas for Project Resources, Track Resources, Analysis Result Resources, Model Resources, and Generated Content Resources (see document for examples).
Data Access Patterns:
Direct Resource Access: Resource ID-based lookup.
Graph-based Navigation: Traverse resource relationships.
Search-based Discovery: Full-text search, semantic similarity search.
Batch Processing Access: Efficient access patterns for large-scale operations.
Scalability Considerations:
Horizontal Scaling: Sharding strategies, distribution of vector operations, partitioning.
Caching Hierarchy: In-memory cache, distributed cache, content-based caching.
Asynchronous Processing: Event-driven updates, background indexing, lazy computation.
Data Lifecycle Management: Automatic archiving, tiered storage, version pruning policies.
V. Training Data Requirements

Generation Model:
Large MIDI Datasets (e.g., Lakh MIDI Dataset).
Audio Datasets (e.g., Million Song Dataset, FreeSound).
Logic Project Files.
Lyrics Datasets.
Production Model:
Audio Recordings with corresponding effect chains.
Sound Design Libraries.
Mixing Sessions.
Mastering Model:
Before-and-After Mastering Pairs.
Mastering Settings.
VI. Inference Methods

Generation Methods:
Sampling Strategies (temperature-controlled sampling, top-k, beam search).
Conditional Generation (harmony-constrained, style-guided, rhythm and tempo conditioning).
Local Inference (Disco-diffusion style, optimized for real-time interaction).
Production and Mastering Methods:
Parameter Prediction (effect parameter regression, chain configuration optimization).
Plugin Integration (Logic Pro API/scripting, non-destructive effect application).
VII. User Interface (UI) Guidelines

Core UI Requirements:
Project Management (import, organization, status visualization).
Analysis Visualization (chord/harmony charts, spectrograms, genre classification).
Editor Integration (MIDI editing, audio editing, Logic Pro integration).
Specialized UI Components:
Generation Control (input type selection, parameter controls, real-time feedback).
Parameter Control (intuitive sliders, preset management, A/B testing).
Process Monitoring (progress indicators, process controls, error messages).
User Experience Principles:
Save and load user presets.
Provide visual feedback for each step.
Enable parameter fine-tuning.
Support keyboard shortcuts.
Clear documentation and help.
VIII. Logic Pro Integration

Plugin Architecture:
Key Design Principles:
Stock Features Only (uses only stock Logic Pro plugins).
External AI Processing (complex AI computations occur in the external Disco-Musica application).
Separation of Concerns (plugin handles Logic Pro integration; external application handles AI).
Hardware Acceleration: Metal framework integration for Apple Silicon; GPU-accelerated processing.
Communication Protocol:
Data Exchange Format: Standardized JSON Structures (project metadata, MIDI modification instructions, audio effect parameters, automation data).
Communication Channels: Socket-based Communication (local TCP/IP socket, binary message framing); File-based Exchange (JSON configuration files, temporary file exchange).
Error Handling: Comprehensive error codes and descriptions; graceful fallback mechanisms.
Plugin Components:
MIDI Manipulation Module: Scripter-based implementation; MIDI generation capabilities.
Audio Effect Control Module: Stock plugin manipulation; parameter automation.
Project Modification Module: Track management; global project parameters.
User Interface Component: Control Panel; parameter adjustment.
Integration with Flows:
Flow #0.1-0.2: Project Management Integration: Data Extraction (plugin extracts data; sends to backend; maintains synchronization).
Flow #1: Project Macro Quantizing Integration: Quantization Application (plugin receives parameters; applies quantization to MIDI and audio).
Flow #2: Generate New Tracks Integration: MIDI Track Creation (plugin receives MIDI data; creates new tracks; applies settings).
Flow #3: Tune Vocals Integration: Pitch Correction Application (plugin applies pitch correction; receives parameters; creates multiple versions).
Flow #4-6: Model Inference Integration: Effect Chain Application; Mastering Integration (plugin receives parameters; configures plugins; generates automation data).
Performance Considerations: Latency Management; Resource Optimization; Error Resilience.
This detailed development guide should provide a solid foundation for developers and code assistants working on the Disco-Musica project. Remember to refer to the specific documents for code snippets, further technical details, and evolving best practices.